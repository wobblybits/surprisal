<html>
  <head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Coiny&family=Fredoka:wght@300..700&family=Major+Mono+Display&family=Noto+Sans:ital,wght@0,100..900;1,100..900&family=Offside&family=Silkscreen:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/styles.css">
    <title>Surprisal Calculator</title>
    <script src="https://unpkg.com/tone"></script>
    <script type="module">
      import { presets, scaleIntervals, notes, models } from "../assets/config.js";

      window.onload = () => {
        const synth = new Tone.Synth().toDestination();
        synth.volume.value = .5;

        const sampler = new Tone.Sampler({
          urls: {
            "A3": "assets/meow.mp3",
          }
        }).toDestination();

        let toneOutput = synth;
        
        let currentVector = [1,2,3,4,5,6,7,8,9,10];
        Tone.Transport.start(0);
        
        let currentSettings = ["THEREMIN", "MAJOR-7", "GPT2"];
        const indicator = document.getElementById('indicator');
        indicator.innerHTML = "<span>" + currentSettings.join('</span><span>') + "</span>";

        synth.set(presets["theremin"]);

        const instruments = Object.keys(presets);
        const scaleGroups = Object.keys(scaleIntervals);
        let scales = {};
        for (const group of scaleGroups) {
          scales = {...scales,  ...scaleIntervals[group]};
        }
        let currentScale = Object.keys(scales)[0];
        
        function fetchFromBackend(text) {
          // Ensure text is not trimmed
          const cleanText = text.trim(); // Only trim leading/trailing whitespace if needed
          fetch('/process/', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({text: cleanText})
          })
          .then(response => response.json())
          .then(data => {
            playMelody(data);
          });
        }

        function fetchFromBackendReverse(text, scale_pitch) {
          try {
            const interval = scales[currentScale].indexOf(scale_pitch);
            fetch('/reverse/', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({text: text, note: interval})
            })
            .then(response => response.json())
            .then(data => {
              console.log(data);
              const userInput = document.getElementById('user-input');
              // Preserve any existing whitespace and add the new token
              const currentText = userInput.value;
              const newText = data.input_text + data.best_token;
              userInput.value = newText;
              userInput.setSelectionRange(data.input_text.length, data.input_text.length + data.best_token.length);
              userInput.focus();
              userInput.blur();
            });
          }
          catch (e) {
            console.log(e);
          }
        }

        function playVector(vector) {
            console.log(vector);
            const notes = vector.map(d => Tone.Frequency("C3").transpose(Math.round(d)));
            console.log(notes);
            var seq = new Tone.Sequence(function(time, note){
                toneOutput.triggerAttackRelease(note, "8n", time);
            }, notes, "4n");
            seq.loop = false;
            seq.start(0);
            Tone.start();
        }
        
        function convertSharpToFlat(note) {
          if (note.indexOf("#") == 1) {
            return notes[notes.indexOf(note[0])+1] + note[2];
          }
          return note;
        }
        
        function convertFlatToSymbol(note) {
          return note.replace("b", "&flat;");
        }

        function convertToScale(surprisals) {
          let converted_pitches = [];
          for(let i = 0; i < surprisals.length; i++) {
            let interval = Math.round(surprisals[i]/2);
            let scaleLength = scales[currentScale].length;
            if (interval > scaleLength - 1) {
              converted_pitches.push(scales[currentScale][scaleLength - 1])
            }
            else {
              converted_pitches.push(scales[currentScale][interval]);
            }
          }
          return converted_pitches
        }

        function convertToContinuous(surprisals) {
          let converted_pitches = [];
          for (let i = 0; i < surprisals.length; i++) {
            // uses midi number to frequency conversion formula
            // similar to chromatic scale, 0 = c3 and 0.5 = d3; surprisals are divided by 2 to squish range
            let new_pitch = Math.pow(2, (48 + surprisals[i]/2 - 69)/12) * 440
            converted_pitches.push(new_pitch.toString());
          }
          // set first note to c3 since 0hz isn't audible
          converted_pitches[0] = 130; 
          console.log(converted_pitches);
          return converted_pitches;
        }
        
        async function playMelody(data) {
            await Tone.start();
            let surprisals = data.surprisals;
            let pitches = [];
            let notes = [];
            if (currentScale === "continuous") {
              pitches = convertToContinuous(surprisals);
              notes = pitches;
              }
            else {
              pitches = convertToScale(surprisals);
              notes = pitches.map(d => Tone.Frequency("C3").transpose(d));
            }
            let durations = data.lengths;
            let volumes = data.frequencies_inverted;
            let delay = Tone.now();
            let offset = delay;
            let timeouts = [];
            //let notes = pitches.map(d => Tone.Frequency("C3").transpose(d))

            const userInput = document.getElementById('user-input');
            let startIndex = 0;
            let endIndex = 0;

            for(let i = 0; i < surprisals.length; i++) {
                timeouts.push(delay);
                endIndex = startIndex + durations[i];
                const s = startIndex;
                const e = endIndex;
                durations[i] /= 8;
                toneOutput.volume.value = volumes[i] / 8;
                
                toneOutput.triggerAttackRelease(notes[i], durations[i], delay);  
                window.setTimeout(() => {
                  // console.log(notes[i].toNote(), convertSharpToFlat(notes[i].toNote()), "start", Tone.Time(durations[i]).toSeconds() * 1000, Tone.Time(timeouts[i]).toSeconds() * 1000); 
                  const key = document.getElementById(convertSharpToFlat(notes[i].toNote()));
                  key.classList.add("highlight");
                  userInput.setSelectionRange(s, e);
                  userInput.focus();
                  window.setTimeout(() => {
                    // console.log(key, notes[i].toNote(), "end");
                    userInput.blur();
                    userInput.setSelectionRange(0, 0);
                    key.classList.remove("highlight");
                  }, Tone.Time(durations[i]).toSeconds() * 900);
                }, (Tone.Time(timeouts[i]).toSeconds() - offset) * 1000);
                delay += durations[i];
                startIndex = endIndex;
            }
        }   
        
        const instrument_container = document.getElementById("instruments");


        for (i in instruments) {
          const button = document.createElement("div");
          button.id = instruments[i];
          button.innerHTML = "<img src='assets/" + instruments[i] + ".png' />";
          button.onclick = (e) => {
            const instrument = button.id;
            const preset = presets[instrument];
            console.log(instrument, preset);
            if (instrument ==="cat") {
              toneOutput = sampler;
            }
            else {
              toneOutput = synth;
              toneOutput.set(preset);
            }
            console.log(instruments, presets, instrument, preset, button, e.target);
            currentSettings[0] = instrument.toUpperCase();
            const indicator = document.getElementById('indicator');
            indicator.innerHTML = "<span>" + currentSettings.join('</span><span>') + "</span>";
          }
          instrument_container.appendChild(button);
        }
        
        const scales_container = document.getElementById("scales");
        for (const groupName in scaleIntervals) {
          const group = document.createElement("div");
          group.id = groupName;
          for (const scaleName in scaleIntervals[groupName]) {     
            const button = document.createElement("div");
            button.id = scaleName;
            button.innerHTML = scaleName.split("-")[0];
            button.onclick = (e) => {
              const selectedScale = button.id;
              const intervals = scales[selectedScale];
              console.log(selectedScale, intervals, button, e.target);
              currentScale = selectedScale;
              currentSettings[1] = selectedScale.toUpperCase();
              const indicator = document.getElementById('indicator');
              indicator.innerHTML = "<span>" + currentSettings.join('</span><span>') + "</span>";
              document.querySelectorAll("#keyboard div").forEach((key, i) => {
                if (scales[selectedScale].includes(i)) {
                  key.classList.remove("disabled");
                }
                else {
                  key.classList.add("disabled");
                }
              });
            }
            group.appendChild(button);
          }
          scales_container.appendChild(group);
        }
        
        const models_container = document.getElementById("models");
        for (i in models) {
          const button = document.createElement("div");
          button.id = models[i];
          button.innerHTML = models[i];
          button.onclick = (e) => {
            const selectedModel = button.id;
            console.log(selectedModel, button, e.target);
            currentSettings[2] = selectedModel.toUpperCase();
            const indicator = document.getElementById('indicator');
            indicator.innerHTML = "<span>" + currentSettings.join('</span><span>') + "</span>";
            fetch(`/model/${selectedModel}`, {
              method: 'GET',
              headers: {
                'Content-Type': 'application/json'
              }
            }).then(response => response.json())
            .then(data => {
              console.log(data);
            });
          }
          models_container.appendChild(button);
        }
        
        let startingNote = "C";
        let startingPitch = notes.indexOf(startingNote);
        let startingOctave = 3;
        
        const piano_container = document.getElementById("keyboard");
        let spacing = 0;
        const numOctaves = 2;
        const numKeysPastOctave = 0;
        
        for (var i = 0; i < numOctaves*12 + numKeysPastOctave; i++) {
          const key = document.createElement("div");
          const note = notes[(i+startingPitch) % notes.length];
          key.id = note + (startingOctave + Math.floor(i/12));
          const scale_pitch = i;
          if (note.length > 1) {
            key.style.bottom = (100*(spacing-.5)/(numOctaves * 7 + numKeysPastOctave)) + "%";
            key.classList.add("black-key");
          }
          else {
            key.style.bottom = (100*spacing/(numOctaves * 7 + numKeysPastOctave)) + "%";
            key.classList.add("white-key");
            spacing++;
          }
          key.innerHTML = convertFlatToSymbol(key.id);
          console.log(currentScale, scales[currentScale], i);
          if (scales[currentScale].includes(i)) {
              key.classList.remove("disabled");
            }
            else {
              key.classList.add("disabled");
            }
          key.onclick = (e) => {
            toneOutput.triggerAttackRelease(key.id, "8n", Tone.now());
            const text = document.getElementById("user-input").value;
            try {
              fetchFromBackendReverse(text, scale_pitch);
            }
            catch (e) {
              console.log(e);
            }
          }
          piano_container.appendChild(key);
        }
        
        const submitButton = document.getElementById("submit");
        submitButton.onclick = (e) => {
          const text = document.getElementById("user-input").value;
          try {
            fetchFromBackend(text);
          }
          catch (e) {
            console.log(e);
          }
        }

        var lastKeyPress = {
          key: null,
          time: null,
          note: null,
        }
        window.onkeydown = (e) => {
          if (document.activeElement.id === "user-input") {
            return;
          }
          if (lastKeyPress.key != e.key && parseInt(e.key) > 0 && parseInt(e.key) < 10) {
            if (lastKeyPress.key !== null) {
              const timeDiff = Math.round((performance.now() - lastKeyPress.time) / 62.5);
              console.log("keyup", lastKeyPress.key, parseInt(lastKeyPress.key), timeDiff, lastKeyPress.note);
              const text = document.getElementById("user-input").value;
              try {
                fetchFromBackendReverse(text, scales[currentScale][parseInt(lastKeyPress.key) - 1]);
                const key = document.getElementById(convertSharpToFlat(lastKeyPress.note.toNote()));
                key.classList.remove("highlight");
              }
              catch (e) {
                console.log(e);
              }
            }
            lastKeyPress.key = e.key;
            lastKeyPress.time = performance.now();
            const pitch = convertToScale([(parseInt(lastKeyPress.key) - 1)*2]);
            const note = Tone.Frequency("C3").transpose(pitch);
            lastKeyPress.note = note;
            toneOutput.triggerAttackRelease(note, "8n", Tone.now());
            const key = document.getElementById(convertSharpToFlat(lastKeyPress.note.toNote()));
            key.classList.add("highlight");
          }
        }
        window.onkeyup = (e) => {
          if (document.activeElement.id === "user-input" || lastKeyPress.key === null) {
            return;
          }
          if (lastKeyPress.key === e.key) {
            const timeDiff = Math.round((performance.now() - lastKeyPress.time) / 62.5);
            console.log(timeDiff);
            const text = document.getElementById("user-input").value;
            try {
              console.log("keyup", lastKeyPress.key, parseInt(lastKeyPress.key), timeDiff, lastKeyPress.note);
              fetchFromBackendReverse(text, scales[currentScale].indexOf(parseInt(lastKeyPress.key) - 1));
              const key = document.getElementById(convertSharpToFlat(lastKeyPress.note.toNote()));
                key.classList.remove("highlight");
            }
            catch (e) {
              console.log(e);
            }
            lastKeyPress.key = null;
            lastKeyPress.time = null;
          }
        }

        /* interactive examples */
        document.getElementById("ex1").onclick = function() {
          document.getElementById("ex1_hidden").classList.remove("hiding");
          document.getElementById("ex1").classList.add("hiding");
        }
        document.getElementById("ex2").onclick = function() {
          document.getElementById("ex2_hidden").classList.remove("hiding");
          document.getElementById("ex2").classList.add("hiding");
        }
        document.getElementById("ex1_hidden").onclick = function() {
          document.getElementById("ex1_hidden").classList.add("hiding");
          document.getElementById("ex1").classList.remove("hiding");
        }
        document.getElementById("ex2_hidden").onclick = function() {
          document.getElementById("ex2_hidden").classList.add("hiding");
          document.getElementById("ex2").classList.remove("hiding");
        }

        document.getElementById("ex1_demo").onclick = function() { 
          document.getElementById("user-input").value = "the man fed the cat some tuna."
          const text = document.getElementById("user-input").value;
          try {
            fetchFromBackend(text);
            window.scrollTo({
              top: 0,
              behavior: "smooth"
            });
          }
          catch (e) {
            console.log(e);
          }
        }

        document.getElementById("ex2_demo").onclick = function() { 
          document.getElementById("user-input").value = "the lawyer presented the cat with a lawsuit."
          const text = document.getElementById("user-input").value;
          try {
            fetchFromBackend(text);
            window.scrollTo({
              top: 0,
              behavior: "smooth"
            });
          }
          catch (e) {
            console.log(e);
          }
        }
      }
    </script>
  </head>
  <body>
    <div id="container">
      <div id="calculator">
        <div id="title">WM7<span style="font-size: .8em; font-family: portfolio">±</span>2</div>
        <div id="display">
          <div id="indicator"></div> 
          <textarea id="user-input" placeholder="Enter text here..."></textarea>
        </div>
        <div id="buttons">
          <div id="scales"></div>
          <div id="models"></div>
          <div id="instruments"></div>
          <div id="translate">
            <div id="submit">Compose ♫</div>
          </div>
        </div>
      </div>
      <div id="keyboard"></div>
    </div>
    <div id="explainer">
      <h1 id="heading">
        The Surprisal Calculator WM7±2
      </h1>
      <h4>Made by David Feil and Elise Kim during a batch at the Recurse Center. <script async defer src="https://www.recurse-scout.com/loader.js?t=62e333ccfa9ade523f73c9755aa46503"></script></h4>

        <p>On this page, we invite you to experiment with the <strong>Surprisal Calculator WM 7±2</strong>, a tool developed to turn the flow of information in human language into melodies. According to <strong>surprisal theory</strong>, the more surprising a sentence is, the longer it takes for the human brain to understand. To see this in action, consider the two sentences below:</p> 

        <div id="examples">
          <div class="example-wrapper">
            <div id="ex1" class="example"> CLICK: sentence 1</div>
            <div id="ex1_hidden" class="example hiding"> The man fed the <strong>cat</strong> some tuna.</div>
          </div>
          <div class="example-wrapper">
            <div id="ex2" class="example"> CLICK: sentence 2</div>
            <div id="ex2_hidden" class="example hiding"> The lawyer presented the <strong>cat</strong> with a lawsuit.</div>
          </div>
        </div>

        <p>Even though the word <em>cat</em> appears in both sentences, it’s far more surprising in the context of the second sentence. Odds are that the second sentence also took you longer to process.</p>
        <p>This seemingly fuzzy notion of “surprisingness” can actually be quantified using a formula first developed by Claude Shannon, the father of information theory. An event's surprisal is calculated as the negative log probability of that event happening in its given context. Applied to language, each word is an event, and the context is the sentence it appears in. Large language models, like GPT, provide a convenient way to calculate word probabilities in context.</p>
        <div id="equation">
          Surprisal(x) = -log<sub>2</sub> P(x | context)
        </div>
        <p>Coming back to our examples: in sentence 1, <em>cat</em> has a low surprisal value, because it appears in a sentence where talking about cats is very probable. In sentence 2, <em>cat</em> has a high surprisal value, because lawyers giving cats lawsuits is improbable.</p>

        <p>Surprisal is a fascinating way to measure how the human brain handles unpredictability in language. However, surprisal numbers aren’t very intuitive to understand on their own. With the <strong>Surprisal Calculator WM 7±2</strong>, you can now input any sentence and hear its surprisal values translated to pitch intervals: a low pitch indicates low surprisal, and a high pitch indicates high surprisal.</p> 

        <div id="demo-wrapper">
          <div id="ex1_demo"> CLICK to hear sentence 1</div>
          <div id="ex2_demo"> CLICK to hear sentence 2</div>
        </div>

        <p>The decision to translate surprisal numbers to musical pitches was not arbitrary. A good, informative sentence is made from a blend of predictability (low surprisal) and spontaneity (high surprisal); the same could also be said of a good melody. We quickly bore of a sentence made only of repetitive, predictable words (“he said that he thought that she said that they think that…”), we also quickly bore of a melody made of the same note, repeated over and over again. The delicate balancing act of predictability and spontaneity that plays out in both language and music seems to hint at a shared cognitive architecture used in both forms of expression.</p>
        <h1 id="how-to-use-the-surprisal-calculator-wm-72">How to use the
        Surprisal Calculator WM 7±2</h1>
        <ul>
        <li><p>Type any sentence in, and press <code>compose</code> to hear its surprisal values.</p></li>
        <li><p>Use the calculator buttons to adjust instrument type, the LLM used to calculate probability scores, and the scale used to match surprisal numbers to pitches.</p></li>
        <li><p>Try typing the beginning of a sentence, then clicking keys on the keyboard. This will generate text with the surprisal values corresponding to the key pitch: experiment with what a very low surprisal or very high surprisal sentence sounds like. We suggest playing a common melody on the keyboard (“jingle bells” is a good candidate). Good melodies, which balance repetition with variety, tend to generate good sentences.</p></li></ul>
        
        <h1>Sources and Code</h1>
        <p><a href="https://github.com/wobblybits/surprisal">Github Repository</a></p>
        <p><a href="https://aclanthology.org/2023.tacl-1.82/">Testing the Predictions of Surprisal Theory in 11 Languages</a> (Wilcox et al., TACL 2023)</p>

        <p><a href="https://doi.org/10.1016/j.cognition.2007.05.006"> Expectation-based syntactic comprehension</a> (Levy, Cognition 2008)</p>
        <p><a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">A mathematical theory of communication</a> (Shannon, 1948)</p>

        <h1>Assets & Models</h1>
        <p><a href="https://huggingface.co/gpt2">GPT2</a>: The smallest (124M parameters) version of the official open-sourced GPT-2 models released from OpenAI.</p>
        <p><a href="https://huggingface.co/HuggingFaceTB/SmolLM-135M">SmolLM</a>: The smallest (135M parameters) of the three models in the SmolLM series of state-of-the-art small language models developed by the Hugging Face research team. These models are built on Cosmo-Corpus, a meticulously curated high-quality training dataset.</p>
        <p><a href="https://huggingface.co/crumb/nano-mistral">Nano Mistral</a>: A small (170M parameters) Mistral model built for general web text completion at extremely low resource use.</p>
        <p><a href="https://huggingface.co/Felladrin/Smol-Llama-101M-Chat-v1">Smol Llama</a>: A small (101M parameters) chat-tuned Llama model based on the work of Peter Szemraj and Vincent Haines.</p>
        <p><a href="https://huggingface.co/KingNish/Qwen2.5-0.5b-Test-ft">Qwen 2.5</a>: A compact yet powerful language model (494M parameters) trained on a subset of the Magpie 300k Database to answer a variety of questions</p>
        <p><a href="https://huggingface.co/google/flan-t5-small">Flan T5</a>: Google's super small (74M parameters)version of the Text-To-Text Transfer Transformer (T5) model, fine-tuned on more than 1000 additional tasks (and more languages!).</p>
        <p>Calculator display font is <a href="https://int10h.org/oldschool-pc-fonts/fontlist/font?portfolio_6x8#-">Portfolio 6x8</a> from the <a href="https://int10h.org/oldschool-pc-fonts/">Old School PC Fonts project</a></p>
        <p>Calculator button font is <a href="https://fonts.google.com/specimen/Fredoka">Fredoka</a> by <a href="https://milenabdesign.com/">Milena Brandão</a></p>
        <p>Instrument icons by <a href="https://www.flaticon.com/authors/ains">Ains</a> and <a href="https://www.flaticon.com/authors/iconpack">IconPack</a> at <a href="https://www.flaticon.com/">Flaticon</a></p>
        <p>Sounds generated with <a href="https://tonejs.github.io/">Tone.js</a></p>
        <p>Cat <a href="https://pixabay.com/sound-effects/cat-meow-8-fx-306184/">sound effect</a> by <a href="https://buymeacoffee.com/bfcmusic">bfcmusic</a></p>
      </div>


  </body>
</html>